---
date: 2020-03-21
status: public
title: mysql机制
tags:
  - JAVA
  - mysql
---

摘要:mysql机制
<!--more-->
# mysql 优化

1，要分析mysql，explain，执行看是否有使用索引
2，不要用like，!=，等等，like %这个走在前面不走索引
3，尽量不要在where里面做表达式操作，判空操作，会导致不走索引

# 事务须知
## 事务实现
```bash
start transaction;

commit;
```
## 实现方式
### 代码方式
```bash
dbc = new DataBaseConnection(); 
Connection con = dbc.getConnection(); 
try { 
	dbc.executeUpdate("delete from xiao_affix where bylawid=" + sID); 
	con.commit();//提交JDBC事务 
	con.setAutoCommit(true);// 恢复JDBC事务的默认提交方式
	dbc.close(); 
}catch (Exception exc) { 
　　 con.rollBack();//回滚JDBC事务 
　　 exc.printStackTrace(); 
　　 dbc.close(); 
　　 return -1; 
} 
```
### 基于 @Transactional 的声明式事务管理
@Transactional(isolation=Isolation.DEFAULT,propagation=Propagation.REQUIRED)

## 有没有用过事务 事务用于什么地方
事务用于缴费的时候要更新多个地方时候，执行报错的时候需要回滚

## 事务注解参数 事务注解有没有什么参数 你需要去配置
RollBackfor NotRollBackfor timeout  事务隔离级别 事务传播行为

一般回滚的话runtimeexception 都会回滚，error也会回滚

## 事务什么时候会回滚  是不是什么报错都会回滚
设置了notrollbackfor 就不会回滚，myisam存储引擎不会回滚事务，innerdb会回滚
如果你抛出的异常是Exception，是不会回滚的，需要抛出一个RuntimeException

## 事务传播行为类型
TransactionDefinition.PROPAGATION_REQUIRED：如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。这是默认值。
TransactionDefinition.PROPAGATION_REQUIRES_NEW：创建一个新的事务，如果当前存在事务，则把当前事务挂起。
TransactionDefinition.PROPAGATION_SUPPORTS：如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。
TransactionDefinition.PROPAGATION_NOT_SUPPORTED：以非事务方式运行，如果当前存在事务，则把当前事务挂起。
TransactionDefinition.PROPAGATION_NEVER：以非事务方式运行，如果当前存在事务，则抛出异常。
TransactionDefinition.PROPAGATION_MANDATORY：如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。
TransactionDefinition.PROPAGATION_NESTED：如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；
如果当前没有事务，则该取值等价于TransactionDefinition.PROPAGATION_REQUIRED。

## 事务四性 
ACID 

原子性：1的操作 任何操作都是1或者0 这个是靠undo日志来实现的

持久性：故障之后任然可以达到恢复 用的是redo日志来实现的

隔离性： 两个事务互相不打扰 是使用读写锁和MVCC来实现的

一致性：数据加起来等于1 是用上面的三个来共同维护的

## redo undo bin log relay log
redo日志 及undo 日志
redo，undo日志都是保存到磁盘里面的

redo日志当你操作的时候，突然程序崩溃了或者宕机，系统断电了
这个时候重启之后，redo日志会同步到数据库里面
 
undo日志是用于回滚操作的，当你事务要执行回滚的时候，你就得通过undo日志来进行回滚

binlog 这个也可以做数据恢复，最新也是先写到binlog里面，然后再写到redo日志里面，最后复制到数据库存储里面

redo log是InnoDB存储引擎层的日志，binlog是MySQL Server层记录的日志， 两者都是记录了某些操作的日志(不是所有)自然有些重复（但两者记录的格式不同）

所以说Oracle 也是有undo 和redo log的，但是没有binlog

mysql主从复制，复制的是binlog 里面的数据   ，写到relay log  然后再写到数据库

### binlog日志录入格式
1，statement，保存用户的所有操作sql，每一条修改数据的sql 都会保存到binlog中
2，row，不记录sql上下文语句，仅仅保存哪条数据被修改
3，mixed，是上面两种的混合

## 事务隔离级别
脏读：就是你提交了数据，我读取了你的数据，这个时候你又回滚了操作，我读取了你的脏数据

不可重复读：就是你读取了数据，我update了数据，这个时候你又读取了数据，数据不一致
A select   -》 B update -》A select  这种应该是不可重复读啊

幻读：你update100条数据，这个时候我又insert了一条，这个时候你查询一次，发现还有一条数据没有update
A select  -》 A update -》B insert -》A select   这种应该就是幻读了

读未提交（read-uncommitted）
读的是别人未提交的数据

读提交（read-committed）
读的是别人commit 数据 还是会出现幻读

可重复读（repeatable-read）
读别人commit数据，然后加间隙锁不给读写

串行化（serializable）
所有的读操作均为当前读，读加读锁 (S锁)，写加写锁 (X锁)。

# mysql 锁机制

## 乐观锁 悲观锁
mysql 里面统一为悲观锁，没有乐观锁，乐观锁是人为的加个状态，例如版本号，更改一下版本号来更改状态，
悲观锁为读写锁，行 表锁，在操作之前先锁住表

## 读写锁
读锁可以并行，写锁又叫排他锁，不可以并行，会造成死锁，互相争夺资源

## 行锁 表锁

## 间隙锁
间隙锁是发生在可重复读的事务隔离级别，在更新插入的时候会自动形成对应的间隙锁，间隙锁是一个分段锁，0-100 ，100-200,200-300...
，当有事务A 要插入的时候会引入间隙锁，这个时候B无法提交，这个就是间隙锁，但是这里会引发死锁问题，当事务B 也引入间隙锁
这个时候就死锁，谁也没法提交事务

[锁机制](https://www.cnblogs.com/lusaisai/p/13400088.html)

## MVCC多版本控制
undo log 中记录某行数据的多个版本的数据

## 当前读  快照读
当前读就是读取当前版本，insert update 这些操作都是当前读，会先获取当前最新数据，然后读取出来加锁，然后再操作
快照读就是读取历史版本，select 就是快照读
当前读的话会进行加锁的机制，在当前读会发生间隙锁的机制，就是不给别人读写，加上排他锁

# 索引
索引用来快速地寻找那些具有特定值的记录。如果没有索引，一般来说执行查询时遍历整张表。

了解索引之前，首先要了解B+树，它是不能在非叶子节点命中的树，它的叶子节点全部是排序的，装载了所有数据

索引的原理很简单，就是把无序的数据变成有序的查询
1、把创建了索引的列的内容进行排序
2、对排序结果生成倒排表
3、在倒排表内容上拼上数据地址链
4、在查询的时候，先拿到倒排表内容，再取出数据地址链，从而拿到具体数据

## 聚簇索引和非聚簇索引
非聚簇索引就是叶子节点存储的是地址，然后地址指向数据
这个时候查询会变慢，为了优化这点问题，加上了聚簇索引，它可以实现叶子节点直接存储数据，不是指针

索引算法有 BTree算法和Hash算法
## 主键索引

## 唯一索引

## 组合索引

这里要说到最左前缀，组合索引要把最常查询的字段排在最前面，ABC三个组成的索引，A开头会走索引，其他开头不走索引

## 普通索引

## 回表
例如我添加了普通索引K ，然后我select a from test where k = 12;
这个时候呢我们会先走k的索引，然后再查询到这条数据的主键，然后再通过主键查询到所有数据，这一个流程称为回表

# 解决mysql两个update 操作的解决方案
## 问题1 超卖问题
不能用快照读，要用当前读，不然数据多的时候会出现超卖问题，就是说解决方案
1，同步机制synchronized
2，要么写sql：update xx set count = count +1 where id=#{id},保证是当前读
3，加锁，加单个锁，变成一个进程 给库存表和余额表添加操作版本号字段，即使用乐观策略，进行cas操作。在每次update的时候先比较版本号是不是自己所获取到的版本号，是，则更新；否，则进行重试。
4，select .... for update语句触发mysql的互斥锁，这样其他线程在查询库存的时候就阻塞，从而达到并发安全。
## 问题2 保证两个update 原子性
1，用事务机制，保证原子性，一个update是另外一个update成功的前提，不然就回滚
 这个方法会出现长事务，数据库锁等待频繁，效率稍微低一些，但是还是可以接受的。
2，捕捉异常，手动进行回滚  执行效率提升，但是代码的复杂度会高很多。


# MySQL中varchar与char的区别及varchar(50)中50代表什么含义？
varchar 是可变长度的   char 是不可变长度的，当同样存储‘hello’这个字符时候，varchar 就只是占用5个字符后面,而char(100)，就算存储5个字符
也会暂用100个字符，用varchar 可以节省存储空间。char(100)在后面都是补空格来填充的，但是order by 的时候会去掉。

# mysql索引数据结构
1，二叉树------要是有序的数字的话，二叉树不合适会一直右边排布下去
2，红黑树------会自动平衡，不会一直排布下去，高度非常高，树的高度非常高，磁盘IO 非常多
3，B树---------控制高度，树的高度比较少，但是没法进行范围查找，B树节点里面存储了数据，这里面要用到很多存储空间
4，B+树--------可以进行范围查找，同时节点里面只是存key，没有存储数据，数据放到叶子页面，这样可以存储很多数据

# 为什么mysql 要增加一个自增索引 用int 类型  为啥不用UUID
1，有利于范围查找
2，增加排序效率
3，提高扫表能力，顺序访问
4，减少分层，如果是UUID的话数据量会比较大
5，这个还要从表的插入有关，自增id的话 每次都在最后面一个插入，这样插入容易
但是用UUID的话 就每次插入都是要计算一遍，看下具体位置是在哪里，这个时候效率就比较低下了
然后当一页插不进去的时候，这个时候要插入额外的一条数据的时候，自增ID 没有这个问题，但是用UUID 会要分裂，分表来插入，这个时候会有问题


# mysql 主从复制原理
原理：
（1）master服务器将数据的改变记录二进制binlog日志，当master上的数据发生改变时，则将其改变写入二进制日志中；
（2）slave服务器会在一定时间间隔内对master二进制日志进行探测其是否发生改变，如果发生改变，则开始一个I/OThread请求master二进制事件
（3）同时主节点为每个I/O线程启动一个dump线程，用于向其发送二进制事件，并保存至从节点本地的中继日志中，从节点将启动SQL线程从中继日志中读取二进制日志，在本地重放，使得其数据和主节点的保持一致，最后I/OThread和SQLThread将进入睡眠状态，等待下一次被唤醒。
也就是说：
## 从库会生成两个线程,一个I/O线程,一个SQL线程;
I/O线程会去请求主库的binlog,并将得到的binlog写到本地的relay-log(中继日志)文件中;
主库会生成一个log dump线程,用来给从库I/O线程传binlog;
SQL线程,会读取relay log文件中的日志,并解析成sql语句逐一执行;


## 分布式事务是怎么做的（seata框架）
1. 二阶段提交协议 (2PC)
	协议中有两类节点：一个中心化协调者 (Coordinator) 节点与 N 个参与者 (Cohort) 节点。
	1），协调者询问所有的参与者是否可以提交事务，所有参与者向协调者投票，同时参与者执行询问发起为止的所有事务操作，并将Undo信息和Redo信息写入本地日志
	2），协调者根据参与者的投票结果，做出是否事务可以全局提交的决定，并通知所有参与者执行该决定，如若一个节点没有回复，那就是通知所有事务回滚。
	缺点 
	1），一旦协调者所在服务器宕机，就会影响整个数据库集群的正常运行
	2），所有的参与者都需要听从协调者的统一调度，期间处于阻塞状态而不能从事其他操作，这样效率极其低下。
	3），两阶段提交协议虽然是分布式数据强一致性所设计，但仍然存在数据不一致性的可能性。比如在第二阶段中，假设协调者发出了事务 commit 通知，
	但是因为网络问题该通知仅被一部分参与者所收到并执行了commit 操作，其余的参与者则因为没有收到通知一直处于阻塞状态，这时候就产生了数据的不一致性。
	
2. 三阶段提交协议 (3PC)
针对两阶段提交存在的问题，三阶段提交协议通过引入一个 预询盘 阶段，以及超时策略来减少整个集群的阻塞时间，提升系统性能。
三阶段提交的三个阶段分别为：预询盘（can_commit）、预提交（pre_commit），以及事务提交（do_commit）。
	1），为甚需要cancommit
	假设有1个协调者，9个参与者。其中有一个参与者不具备执行该事务的能力。
	协调者发出prepare消息之后，其余参与者都将资源锁住，执行事务，写入undo和redo日志。
	协调者收到相应之后，发现有一个参与者不能参与。所以，又出一个roolback消息。其余8个参与者，又对消息进行回滚。这样子，是不是做了很多无用功？
	所以，引入can-Commit阶段，主要是为了在预执行之前，保证所有参与者都具备可执行条件，从而减少资源浪费。
	2)，这种方案解决了服务协调者单点问题，在cancommit 之后 precommit 之后 所有节点在docommit阶段要是超时没有收到协调者信息的时候会默认提交事务
	因为在precommit阶段已经知道所有节点都同意提交事务了
3. 基于消息的分布式事务
	基于rabbitmq的消息队列，每次发送的时候先执行A，然后再发送给rabbitmq 这样就不会消息丢失
	
# 怎么处理慢查询
1，先开启慢查询，并且把慢查询的日志打印，开启的是slow_query_log
2，explain去查看语句，看下语句有无走索引
3，优化sql，不要去用like及一些比较复杂的运算